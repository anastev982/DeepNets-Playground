{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3dadb0-b954-472d-81a3-3547b2916b3d",
   "metadata": {},
   "source": [
    "Compare multiple ML models per gene on LN_IC50 target.\n",
    "\n",
    "Extension of the single-model script:\n",
    "- loads Parquet with pandas (fast metadata / adequate for medium subsets)\n",
    "- evaluates multiple models per gene (train/test split)\n",
    "- saves long & wide CSVs\n",
    "- saves one bar plot per model (Top-N genes with lowest MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff14d6b-febf-4d49-b7c6-d0581fc9d744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY: /home/paunica/miniconda/bin/python\n",
      "JAVA_HOME: None\n",
      "/bin/bash: java: command not found\n",
      "java not found\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, pathlib, sys\n",
    "\n",
    "def set_java_home():\n",
    "    # kandidati: (1) sistemska java, (2) conda JAVA u $CONDA_PREFIX\n",
    "    candidates = []\n",
    "    java = shutil.which(\"java\")\n",
    "    if java:\n",
    "        candidates.append(str(pathlib.Path(java).resolve().parents[1]))  # .../bin/java -> JDK root\n",
    "    cp = os.environ.get(\"CONDA_PREFIX\")\n",
    "    if cp:\n",
    "        candidates.append(cp)\n",
    "\n",
    "    for c in candidates:\n",
    "        if c and (pathlib.Path(c)/\"bin\"/\"java\").exists():\n",
    "            os.environ[\"JAVA_HOME\"] = c\n",
    "            os.environ[\"PATH\"] = os.pathsep.join([str(pathlib.Path(c)/\"bin\"), os.environ[\"PATH\"]])\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "jh = set_java_home()\n",
    "print(\"PY:\", sys.executable)\n",
    "print(\"JAVA_HOME:\", jh or os.environ.get(\"JAVA_HOME\"))\n",
    "!java -version || echo \"java not found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15d88f8-5c3c-4fef-af21-9ca1722ae7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProject2-ML\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.ui.showConsoleProgress\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfalse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      5\u001b[39m DF_PATH = Path(\u001b[33m\"\u001b[39m\u001b[33mdata/california_housing.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/lib/python3.13/site-packages/pyspark/sql/session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/lib/python3.13/site-packages/pyspark/core/context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/lib/python3.13/site-packages/pyspark/core/context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/lib/python3.13/site-packages/pyspark/core/context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/lib/python3.13/site-packages/pyspark/java_gateway.py:111\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    117\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Project2-ML\").config(\"spark.ui.showConsoleProgress\",\"false\").getOrCreate()\n",
    "\n",
    "from pathlib import Path\n",
    "DF_PATH = Path(\"data/california_housing.parquet\")\n",
    "if not DF_PATH.exists():\n",
    "    from sklearn.datasets import fetch_california_housing\n",
    "    import pandas as pd\n",
    "    DF_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    pdf = fetch_california_housing(as_frame=True).frame\n",
    "    spark.createDataFrame(pdf).write.mode(\"overwrite\").parquet(str(DF_PATH))\n",
    "\n",
    "df = spark.read.parquet(str(DF_PATH))\n",
    "df.printSchema(); df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a7107-21be-407f-9c58-561833d29d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, corr, col\n",
    "\n",
    "df.select(avg(\"MedInc\").alias(\"avg_income\")).show()\n",
    "df.select(corr(\"MedInc\",\"MedHouseVal\").alias(\"corr_income_price\")).show()\n",
    "\n",
    "df.createOrReplaceTempView(\"cal\")\n",
    "spark.sql(\"\"\"\n",
    "  SELECT ROUND(Latitude) AS lat, COUNT(*) AS n, AVG(MedHouseVal) AS avg_price\n",
    "  FROM cal GROUP BY ROUND(Latitude) ORDER BY avg_price DESC LIMIT 10\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b6fb0-d2be-4559-b18b-a2f9a3ed2af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Primer imputacije i jednostavnog feature-a:\n",
    "df_prep = df.na.fill({\"Population\": 0})\n",
    "df_prep = df_prep.withColumn(\"flag_old\", when(col(\"HouseAge\") > 30, 1).otherwise(0))\n",
    "\n",
    "# Sve numeričke + novi feature\n",
    "num_cols = [\"MedInc\",\"HouseAge\",\"AveRooms\",\"AveBedrms\",\"Population\",\"AveOccup\",\"Latitude\",\"Longitude\",\"flag_old\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=num_cols, outputCol=\"features_raw\")\n",
    "scaler    = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=False, withStd=True)\n",
    "\n",
    "fe_model = Pipeline(stages=[assembler, scaler]).fit(df_prep)\n",
    "df_fe = fe_model.transform(df_prep)\n",
    "\n",
    "df_fe.select(\"features\").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404a57e-c59c-4a64-b370-3ca3a975dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    try:\n",
    "        # pokušaj čitanja mog fajla\n",
    "        df = pd.read_parquet(DATA_PATH)\n",
    "        print(f\"Loaded {DATA_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Parquet file not found, using California housing dataset instead.\")\n",
    "        data = fetch_california_housing(as_frame=True)\n",
    "        df = data.frame\n",
    "        # target se u mom projektu zvao LN_IC50 → ovde ga preslikavam\n",
    "        df.rename(columns={\"MedHouseVal\": \"LN_IC50\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22f07b-ff90-4b93-9bfc-91328e9b91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# binarizuj target po medijani\n",
    "median_val = df.approxQuantile(\"MedHouseVal\", [0.5], 0.01)[0]\n",
    "df_cls = df_fe.withColumn(\"label\", (col(\"MedHouseVal\") >= median_val).cast(\"int\"))\n",
    "\n",
    "train_df, test_df = df_cls.select(\"features\",\"label\").randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
    "lr_model = lr.fit(train_df)\n",
    "pred = lr_model.transform(test_df)\n",
    "\n",
    "auc_roc = BinaryClassificationEvaluator(metricName=\"areaUnderROC\", labelCol=\"label\").evaluate(pred)\n",
    "auc_pr  = BinaryClassificationEvaluator(metricName=\"areaUnderPR\",  labelCol=\"label\").evaluate(pred)\n",
    "f1      = MulticlassClassificationEvaluator(metricName=\"f1\",       labelCol=\"label\").evaluate(pred)\n",
    "acc     = MulticlassClassificationEvaluator(metricName=\"accuracy\", labelCol=\"label\").evaluate(pred)\n",
    "print(f\"AUC-ROC={auc_roc:.3f}  AUC-PR={auc_pr:.3f}  F1={f1:.3f}  ACC={acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4024106-d7bc-44d6-9572-8b1b65eceba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d8f0a-e46c-4371-84d5-27dd718799f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215fe95-2fd1-4664-96b3-9a1f28385361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (WSL)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
