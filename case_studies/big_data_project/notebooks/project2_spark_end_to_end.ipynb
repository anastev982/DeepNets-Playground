{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a9ed0d-b6a6-403f-8b28-d09705110cc0",
   "metadata": {},
   "source": [
    "# Project 2 — PySpark Pipeline\n",
    "End-to-end ML with PySpark:  \n",
    "- Load California Housing dataset  \n",
    "- Feature Engineering  \n",
    "- Binary classification (high vs low price)  \n",
    "- Metrics: AUC-ROC, AUC-PR, ACC, F1  \n",
    "- ROC & PR plots + Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b24d8db-909b-4bb1-8836-bd69e616360d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MedInc: double (nullable = true)\n",
      " |-- HouseAge: double (nullable = true)\n",
      " |-- AveRooms: double (nullable = true)\n",
      " |-- AveBedrms: double (nullable = true)\n",
      " |-- Population: double (nullable = true)\n",
      " |-- AveOccup: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- MedHouseVal: double (nullable = true)\n",
      "\n",
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----------+\n",
      "|MedInc|HouseAge|AveRooms          |AveBedrms         |Population|AveOccup          |Latitude|Longitude|MedHouseVal|\n",
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----------+\n",
      "|8.3252|41.0    |6.984126984126984 |1.0238095238095237|322.0     |2.5555555555555554|37.88   |-122.23  |4.526      |\n",
      "|8.3014|21.0    |6.238137082601054 |0.9718804920913884|2401.0    |2.109841827768014 |37.86   |-122.22  |3.585      |\n",
      "|7.2574|52.0    |8.288135593220339 |1.073446327683616 |496.0     |2.8022598870056497|37.85   |-122.24  |3.521      |\n",
      "|5.6431|52.0    |5.8173515981735155|1.0730593607305936|558.0     |2.547945205479452 |37.85   |-122.25  |3.413      |\n",
      "|3.8462|52.0    |6.281853281853282 |1.0810810810810811|565.0     |2.1814671814671813|37.85   |-122.25  |3.422      |\n",
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Start Spark and load data\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Project2-PySpark-Compact\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# sklearn -> pandas -> spark\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load California Housing\n",
    "pdf = fetch_california_housing(as_frame=True).frame\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Verify the dataset is loaded\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a55bd44-1e2a-4d64-97f8-094b91781806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.BufferedWriter name=5>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paunica/miniconda/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| corr_income_price|\n",
      "+------------------+\n",
      "|0.6880752079585483|\n",
      "+------------------+\n",
      "\n",
      "+---------+------------------+\n",
      "|lat_round|         avg_price|\n",
      "+---------+------------------+\n",
      "|     34.0| 2.295873273842111|\n",
      "|     37.0|2.2478975830815684|\n",
      "|     38.0|2.2210915330417875|\n",
      "|     33.0| 1.939257056874655|\n",
      "|     39.0|1.3371683030726256|\n",
      "+---------+------------------+\n",
      "only showing top 5 rows\n",
      "+---------+------------------+\n",
      "|lon_round|         avg_price|\n",
      "+---------+------------------+\n",
      "|   -122.0|2.4468768492031034|\n",
      "|   -118.0| 2.345842784979286|\n",
      "|   -123.0| 2.105219970501476|\n",
      "|   -119.0|1.9354980487804871|\n",
      "|   -117.0|1.6914726552128074|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import avg, corr, round as sround, col, desc\n",
    "\n",
    "# Correlation\n",
    "df.select(corr(\"MedInc\",\"MedHouseVal\").alias(\"corr_income_price\")).show()\n",
    "\n",
    "# Aggregation: mean(price by rounded Latitude)\n",
    "df.groupBy(sround(\"Latitude\").alias(\"lat_round\")) \\\n",
    "  .agg(avg(\"MedHouseVal\").alias(\"avg_price\")) \\\n",
    "  .orderBy(desc(\"avg_price\")).show(5)\n",
    "\n",
    "# SQL example\n",
    "df.createOrReplaceTempView(\"cal\")\n",
    "spark.sql(\"\"\"\n",
    "  SELECT ROUND(Longitude) AS lon_round, AVG(MedHouseVal) AS avg_price\n",
    "  FROM cal\n",
    "  GROUP BY ROUND(Longitude)\n",
    "  ORDER BY avg_price DESC\n",
    "  LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d22a420-4b13-41db-8fbc-79c4697c232e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-------------+\n",
      "|lon_round|         avg_price|rank_by_price|\n",
      "+---------+------------------+-------------+\n",
      "|   -122.0|2.4468768492031034|            1|\n",
      "|   -118.0| 2.345842784979286|            2|\n",
      "|   -123.0| 2.105219970501476|            3|\n",
      "|   -119.0|1.9354980487804871|            4|\n",
      "|   -117.0|1.6914726552128074|            5|\n",
      "|   -120.0|1.3619122203947367|            6|\n",
      "|   -121.0|1.3479995067264572|            7|\n",
      "|   -116.0|0.9869956387665196|            8|\n",
      "|   -124.0|0.9675588235294115|            9|\n",
      "|   -114.0|0.7816666666666667|           10|\n",
      "+---------+------------------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Window ranking\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "# Group by rounded length + mean price\n",
    "df_win = df.groupBy(sround(\"Longitude\").alias(\"lon_round\")) \\\n",
    "           .agg(avg(\"MedHouseVal\").alias(\"avg_price\"))\n",
    "\n",
    "# RANKING by avg_price (descending) — use existing columns\n",
    "w = Window.orderBy(desc(\"avg_price\"))\n",
    "df_win = df_win.withColumn(\"rank_by_price\", dense_rank().over(w))\n",
    "\n",
    "## Show top-10 buckets\n",
    "df_win.orderBy(desc(\"avg_price\")).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc421464-d628-40de-bf58-7b9c526a540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[4.382095394195215,3.257702301608303,2.8228125480951682,2.1603419907541546,0.2843362208866197,0.24605655309533112,17.734477624640366,-61.00726959606932,7.847538740944807,3.244089868349824,2.0063728901333038]|\n",
      "|[4.369567902917906,1.6685792276530331,2.521301756615351,2.0507664641049574,2.120159212263273,0.2031418986718503,17.7251141200867,-61.00227840981422,10.57275264633217,2.8892979459056067,0.0]                  |\n",
      "|[3.8200426552914464,4.131719992283701,3.349860792340826,2.2650806840386486,0.4379837439744204,0.2698099860028391,17.72043236780987,-61.01226078232441,8.432863043801259,3.3560156847288676,2.0063728901333038] |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import when, log1p\n",
    "\n",
    "# Numeric columns for imputation (median)\n",
    "num = [\"MedInc\",\"HouseAge\",\"AveRooms\",\"AveBedrms\",\"Population\",\"AveOccup\",\"Latitude\",\"Longitude\"]\n",
    "\n",
    "# Median imputation for robustness\n",
    "imp = Imputer(strategy=\"median\", inputCols=num, outputCols=[c+\"_imp\" for c in num])\n",
    "\n",
    "# Custom FE transformer: log1p(Population), interaction, flag by age\n",
    "class FE(Transformer):\n",
    "    def _transform(self, ds):\n",
    "        return (ds\n",
    "            .withColumn(\"Population_log1p\", log1p(col(\"Population_imp\")))\n",
    "            .withColumn(\"IncomeRooms_inter\", col(\"MedInc_imp\")*col(\"AveRooms_imp\"))\n",
    "            .withColumn(\"flag_old\", when(col(\"HouseAge_imp\")>30, 1).otherwise(0)))\n",
    "# Aggregate numeric + engineering features in one vector\n",
    "fe_cols = [c+\"_imp\" for c in num] + [\"Population_log1p\",\"IncomeRooms_inter\",\"flag_old\"]\n",
    "va = VectorAssembler(inputCols=fe_cols, outputCol=\"features_raw\", \n",
    "                     handleInvalid=\"skip\")\n",
    "\n",
    "# Scale features (withMean=False is safe for sparse/dense vectors)\n",
    "sc = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=False, withStd=True)\n",
    "\n",
    "# Build and fit the pipeline\n",
    "pipe = Pipeline(stages=[imp, FE(), va, sc]).fit(df)\n",
    "\n",
    "# Apply FE pipeline\n",
    "data_all = pipe.transform(df)\n",
    "data_all.select(\"features\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dc8c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.BufferedWriter name=5>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paunica/miniconda/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline  AUC-ROC=0.919  AUC-PR=0.925  ACC=0.837  F1=0.837\n"
     ]
    }
   ],
   "source": [
    "# Baseline LR (Occam's razor) and core metrics\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#Baseline model\n",
    "lr_base = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=80)\n",
    "model_base = lr_base.fit(train)\n",
    "pred_base = model_base.transform(test)\n",
    "\n",
    "# Define evaluater\n",
    "evaluator_roc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "evaluator_pr  = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderPR\")\n",
    "m_acc = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "m_f1  = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\")\n",
    "\n",
    "print(f\"Baseline  AUC-ROC={evaluator_roc.evaluate(pred_base):.3f}  \"\n",
    "      f\"AUC-PR={evaluator_pr.evaluate(pred_base):.3f}  \"\n",
    "      f\"ACC={m_acc.evaluate(pred_base):.3f}  F1={m_f1.evaluate(pred_base):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dca3ac5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      4\u001b[39m paramGrid = (ParamGridBuilder()\n\u001b[32m      5\u001b[39m              .addGrid(lr_base.regParam, [\u001b[32m0.0\u001b[39m, \u001b[32m0.001\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m0.1\u001b[39m])\n\u001b[32m      6\u001b[39m              .addGrid(lr_base.elasticNetParam, [\u001b[32m0.0\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m1.0\u001b[39m])   \u001b[38;5;66;03m# 0=L2, 1=L1\u001b[39;00m\n\u001b[32m      7\u001b[39m              .build())\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Define cross-validator\u001b[39;00m\n\u001b[32m     10\u001b[39m cv = CrossValidator(estimator=lr_base,\n\u001b[32m     11\u001b[39m                     estimatorParamMaps=paramGrid,\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m                     evaluator=\u001b[43mevaluator\u001b[49m,\n\u001b[32m     13\u001b[39m                     numFolds=\u001b[32m3\u001b[39m,\n\u001b[32m     14\u001b[39m                     parallelism=\u001b[32m2\u001b[39m)\n\u001b[32m     17\u001b[39m cvModel = cv.fit(train)\n\u001b[32m     18\u001b[39m pred_tuned = cvModel.transform(test)\n",
      "\u001b[31mNameError\u001b[39m: name 'evaluator' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Define parametar grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr_base.regParam, [0.0, 0.001, 0.01, 0.1])\n",
    "             .addGrid(lr_base.elasticNetParam, [0.0, 0.5, 1.0])   # 0=L2, 1=L1\n",
    "             .build())\n",
    "\n",
    "# Define cross-validator\n",
    "cv = CrossValidator(estimator=lr_base,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3,\n",
    "                    parallelism=2)\n",
    "\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "pred_tuned = cvModel.transform(test)\n",
    "\n",
    "print(f\"Tuned     AUC-ROC={evaluator_roc.evaluate(pred_tuned):.3f}  \"\n",
    "      f\"AUC-PR={evaluator_pr.evaluate(pred_tuned):.3f}  \"\n",
    "      f\"ACC={m_acc.evaluate(pred_tuned):.3f}  F1={m_f1.evaluate(pred_tuned):.3f}\")\n",
    "\n",
    "# Best params\n",
    "best_lr = cvModel.bestModel\n",
    "print(\"Best regParam:\", best_lr._java_obj.getRegParam())\n",
    "print(\"Best elasticNetParam:\", best_lr._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae069b20-2642-4e55-acc5-716b868cd28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test: 16563 4077\n"
     ]
    }
   ],
   "source": [
    "# Define label on median price(target value)\n",
    "\n",
    "from pyspark.sql.functions import col as scol\n",
    "try:\n",
    "    median_val = df.approxQuantile(\"MedHouseVal\", [0.5], 0.01)[0]\n",
    "except Exception:\n",
    "    median_val = float(pdf[\"MedHouseVal\"].median())\n",
    "    print(\"Using median from Pandas:\", median_val)\n",
    "\n",
    "# Add label column and select features and labels\n",
    "data = (data_all\n",
    "        .withColumn(\"label\", (scol(\"MedHouseVal\")>=median_val).cast(\"int\"))\n",
    "        .select(\"features\",\"label\"))\n",
    "\n",
    "train, test = data.randomSplit([0.8,0.2], seed=42)\n",
    "print(\"train/test:\", train.count(), test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e9695-f742-4be7-80e4-3c6a49c39837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC=0.919  AUC-PR=0.925  ACC=0.837  F1=0.837\n"
     ]
    }
   ],
   "source": [
    "# Models + metrics (Train Logistic Regression + compute metrics)\n",
    "# Train a simple baseline classifier (Occam's razor)\n",
    "# Evaluate with standard metrics (Accuracy, F1, ROC AUC, PR AUC)\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "model = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=80).fit(train)\n",
    "pred  = model.transform(test)\n",
    "\n",
    "# Classification metrics\n",
    "auc_roc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\").evaluate(pred)\n",
    "auc_pr  = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderPR\").evaluate(pred)\n",
    "acc     = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\").evaluate(pred)\n",
    "f1      = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\").evaluate(pred)\n",
    "print(f\"AUC-ROC={auc_roc:.3f}  AUC-PR={auc_pr:.3f}  ACC={acc:.3f}  F1={f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128ef36-2aee-45d7-8659-0588f8b9bcae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vector_to_array\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m pred = \u001b[43mpred_base\u001b[49m\n\u001b[32m      9\u001b[39m pp = pred.select(\n\u001b[32m     10\u001b[39m     scol(\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     11\u001b[39m     vector_to_array(scol(\u001b[33m\"\u001b[39m\u001b[33mprobability\u001b[39m\u001b[33m\"\u001b[39m)).getItem(\u001b[32m1\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mp\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     12\u001b[39m     scol(\u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33myhat\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m ).toPandas()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ROC\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'pred_base' is not defined"
     ]
    }
   ],
   "source": [
    "# ROC curve and confusion matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "pred = pred_tuned\n",
    "\n",
    "pp = pred.select(\n",
    "    scol(\"label\").cast(\"int\").alias(\"y\"),\n",
    "    vector_to_array(scol(\"probability\")).getItem(1).alias(\"p\"),\n",
    "    scol(\"prediction\").cast(\"int\").alias(\"yhat\")\n",
    ").toPandas()\n",
    "\n",
    "# ROC\n",
    "fpr, tpr, _ = roc_curve(pp[\"y\"], pp[\"p\"])\n",
    "plt.plot(fpr, tpr, label=f\"AUC={auc(fpr,tpr):.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (test)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(pp[\"y\"], pp[\"yhat\"])\n",
    "ConfusionMatrixDisplay(cm).plot(values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix (test)\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af515229",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluator_roc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtuning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParamGridBuilder, CrossValidator\n\u001b[32m      4\u001b[39m paramGrid = (ParamGridBuilder()\n\u001b[32m      5\u001b[39m              .addGrid(lr_base.regParam, [\u001b[32m0.0\u001b[39m, \u001b[32m0.001\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m0.1\u001b[39m])\n\u001b[32m      6\u001b[39m              .addGrid(lr_base.elasticNetParam, [\u001b[32m0.0\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m1.0\u001b[39m])\n\u001b[32m      7\u001b[39m              .build())\n\u001b[32m      9\u001b[39m cv = CrossValidator(estimator=lr_base,\n\u001b[32m     10\u001b[39m                     estimatorParamMaps=paramGrid,\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m                     evaluator=\u001b[43mevaluator_roc\u001b[49m,\n\u001b[32m     12\u001b[39m                     numFolds=\u001b[32m3\u001b[39m,\n\u001b[32m     13\u001b[39m                     parallelism=\u001b[32m2\u001b[39m)\n\u001b[32m     15\u001b[39m cvModel = cv.fit(train)\n\u001b[32m     16\u001b[39m pred_tuned = cvModel.transform(test)\n",
      "\u001b[31mNameError\u001b[39m: name 'evaluator_roc' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for LR (regParam, elasticNetParam)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr_base.regParam, [0.0, 0.001, 0.01, 0.1])\n",
    "             .addGrid(lr_base.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator=lr_base,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator_roc,\n",
    "                    numFolds=3,\n",
    "                    parallelism=2)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "pred_tuned = cvModel.transform(test)\n",
    "\n",
    "print(f\"Tuned     AUC-ROC={evaluator_roc.evaluate(pred_tuned):.3f}  \"\n",
    "      f\"AUC-PR={evaluator_pr.evaluate(pred_tuned):.3f}  \"\n",
    "      f\"ACC={m_acc.evaluate(pred_tuned):.3f}  F1={m_f1.evaluate(pred_tuned):.3f}\")\n",
    "\n",
    "best_lr = cvModel.bestModel\n",
    "print(\"Best regParam:\", best_lr._java_obj.getRegParam())\n",
    "print(\"Best elasticNetParam:\", best_lr._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d611e50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_tuned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vector_to_array\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m pred = \u001b[43mpred_tuned\u001b[49m  \u001b[38;5;66;03m# or pred_base if you want to plot baseline\u001b[39;00m\n\u001b[32m      8\u001b[39m pp = pred.select(\n\u001b[32m      9\u001b[39m     scol(\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     10\u001b[39m     vector_to_array(scol(\u001b[33m\"\u001b[39m\u001b[33mprobability\u001b[39m\u001b[33m\"\u001b[39m)).getItem(\u001b[32m1\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mp\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     11\u001b[39m     scol(\u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33myhat\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m ).toPandas()\n\u001b[32m     14\u001b[39m fpr, tpr, _ = roc_curve(pp[\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m], pp[\u001b[33m\"\u001b[39m\u001b[33mp\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'pred_tuned' is not defined"
     ]
    }
   ],
   "source": [
    "# Optional plots (requires Spark >= 3.1 for vector_to_array)\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "pred = pred_tuned  # or pred_base if you want to plot baseline\n",
    "\n",
    "pp = pred.select(\n",
    "    scol(\"label\").cast(\"int\").alias(\"y\"),\n",
    "    vector_to_array(scol(\"probability\")).getItem(1).alias(\"p\"),\n",
    "    scol(\"prediction\").cast(\"int\").alias(\"yhat\")\n",
    ").toPandas()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(pp[\"y\"], pp[\"p\"])\n",
    "plt.plot(fpr, tpr, label=f\"AUC={auc(fpr,tpr):.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (test)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "cm = confusion_matrix(pp[\"y\"], pp[\"yhat\"])\n",
    "ConfusionMatrixDisplay(cm).plot(values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix (test)\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab7009-b1a5-4c5d-baee-4af09f08cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd87a864-e148-4c10-8338-76a1baf8defc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (WSL)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
